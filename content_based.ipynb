{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "content_based.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshumanJain101/Recommendation-System/blob/main/content_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03gHFK-2aD41"
      },
      "source": [
        "from collections import Counter, defaultdict\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "from math import sqrt\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import urllib.request\r\n",
        "import zipfile"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhUrol5aGIS"
      },
      "source": [
        "def cosine_sim(a, b):\r\n",
        "      # The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\r\n",
        "      # where ||a|| indicates the Euclidean normal of vector a.\r\n",
        "\r\n",
        "    a = a.toarray()\r\n",
        "    b = b.toarray()\r\n",
        "    return (np.dot(a,b.T)) / (np.sqrt(np.sum(np.square(a))) * np.sqrt(np.sum(np.square(b))))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHDaIGklaErM"
      },
      "source": [
        "def tokenize_string(my_string):\r\n",
        "#here we replace all \"|\" this \",\"\r\n",
        "    return re.findall('[\\w\\-]+', my_string.lower())"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEBfKJg6aFEG"
      },
      "source": [
        "def tokenize(movies):\r\n",
        "    movies['tokens'] = [tokenize_string(genre) for genre in movies['genres']]\r\n",
        "\r\n",
        "    return movies"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXekHDZzaFbQ"
      },
      "source": [
        "def make_vocab(movies):\r\n",
        "    # tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\r\n",
        "    # where:\r\n",
        "    # i is a term\r\n",
        "    # d is a document (movie)\r\n",
        "    # tf(i, d) is the frequency of term i in document d\r\n",
        "    # max_k tf(k, d) is the maximum frequency of any term in document d\r\n",
        "    # N is the number of documents (movies)\r\n",
        "    # df(i) is the number of unique documents containing term i\r\n",
        "   \r\n",
        "    #creating a vocab of all the unique genres\r\n",
        "    vocab = {movie_tokens:idx for idx, movie_tokens in enumerate(sorted(np.unique(np.concatenate(movies.tokens))))}\r\n",
        "\r\n",
        "    # creating df\r\n",
        "    df = defaultdict(int)\r\n",
        "    for movie_genre in movies.tokens:\r\n",
        "        for genre in vocab:\r\n",
        "            if genre in movie_genre:\r\n",
        "                df[genre]+=1\r\n",
        "\r\n",
        "    all_csr = []\r\n",
        "    for idx, movie in enumerate(movies.tokens):\r\n",
        "        colmn, data, row = [], [], []\r\n",
        "        tf = Counter(movie)     # tf\r\n",
        "        max_k = tf.most_common(1)[0][1]\r\n",
        "        for genre, freq in tf.items():\r\n",
        "            if genre in vocab:\r\n",
        "                colmn.append(vocab[genre])\r\n",
        "                data.append((freq/max_k)*math.log10(len(movies)/df[genre])) # tf-idf\r\n",
        "                X = csr_matrix((np.asarray(data), (np.zeros(shape=(len(data))), np.asarray(colmn))), shape=(1, len(vocab)))\r\n",
        "\r\n",
        "        all_csr.append(X)\r\n",
        "\r\n",
        "    movies['features'] = all_csr\r\n",
        "\r\n",
        "    return movies, vocab"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRhNaTs5aFw6"
      },
      "source": [
        "#rendom split of dataset for training and testing\r\n",
        "# print(ratings)\r\n",
        "def train_test(ratings):\r\n",
        "    test = set(range(len(ratings))[::1000])\r\n",
        "    # print(\"\\n\\n\\n\")\r\n",
        "    # print(test)\r\n",
        "    # print(\"\\n\\n\\n\")\r\n",
        "    train = sorted(set(range(len(ratings))) - test)\r\n",
        "    # print(train)\r\n",
        "    # print(\"\\n\\n\\n\")\r\n",
        "    test = sorted(test)\r\n",
        "    return ratings.iloc[train], ratings.iloc[test]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb3Rw_qlaGk8"
      },
      "source": [
        "def predicting_main_fxn(movies, ratings_train, ratings_test):\r\n",
        "    \r\n",
        "    predictions = []\r\n",
        "    for test_userid, test_movieid in zip(ratings_test.userId, ratings_test.movieId):\r\n",
        "        # got the test userid & test movieid\r\n",
        "        # print(\"Getting for\", test_userid, test_movieid)\r\n",
        "        \r\n",
        "        weight_ratings = []\r\n",
        "        weights = []\r\n",
        "        target_user_ratings = []\r\n",
        "        for idx, train_user in ratings_train.loc[ratings_train.userId == test_userid, 'movieId': 'rating'].iterrows():\r\n",
        "            # got the ratings and movieId for the test userId\r\n",
        "            # print(idx)\r\n",
        "            # print(\"\\n\\n\\n\")\r\n",
        "            # print(train_user)\r\n",
        "            # print((movies.loc[movies.movieId == int(train_user.movieId)].features.values[0]))\r\n",
        "            \r\n",
        "            cos_sim_weight = cosine_sim(movies.loc[movies.movieId == int(train_user.movieId)].features.values[0],\r\n",
        "                                        movies.loc[movies.movieId == int(test_movieid)].features.values[0]) #finding cosine similarity\r\n",
        "\r\n",
        "            # print(weight_ratings)\r\n",
        "            weight_ratings.append(train_user.rating * cos_sim_weight)\r\n",
        "            weights.append(cos_sim_weight)\r\n",
        "            target_user_ratings.append(train_user.rating)\r\n",
        "\r\n",
        "        if np.count_nonzero(weights) > 0:\r\n",
        "            predictions.append(np.sum(weight_ratings)/np.sum(weights))\r\n",
        "        else:\r\n",
        "            predictions.append(ratings_train.loc[ratings_train.userId == test_userid, 'rating'].mean())\r\n",
        "\r\n",
        "    return np.asarray(predictions)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5GK1AfWagX7"
      },
      "source": [
        "#finding error\r\n",
        "def error(predictions, ratings_test):\r\n",
        "    return np.abs(predictions - np.array(ratings_test.rating)).mean() #returning the value of error"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xO-j-7kPNaG"
      },
      "source": [
        "def get_rmse(pred, actual):\r\n",
        "    # Ignore zero terms.\r\n",
        "    # pred = pred[actual.nonzero()].flatten()\r\n",
        "    # actual = actual[actual.nonzero()].flatten()\r\n",
        "    return sqrt(mean_squared_error(pred, actual.rating))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSDTLmD3ajLz",
        "outputId": "2595df9d-2336-484f-d0c2-fd1fc17d957d"
      },
      "source": [
        "ratings = pd.read_csv('ratings.csv')\r\n",
        "movies = pd.read_csv('movies.csv')\r\n",
        "# 1st step I was thinking of to have genure list\r\n",
        "movies = tokenize(movies)\r\n",
        "# 2nd step I was thinking to include tf-idf as well as an id corresponding to all genures\r\n",
        "movies, vocab = make_vocab(movies)\r\n",
        "#print('vocab:')\r\n",
        "#print(sorted(vocab.items())[:10])\r\n",
        "# print(ratings)\r\n",
        "# print(\"\\n\\n\\n\")\r\n",
        "ratings_train, ratings_test = train_test(ratings)\r\n",
        "# print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\r\n",
        "predictions = predicting_main_fxn(movies, ratings_train, ratings_test)\r\n",
        "print('MAE=%f' %error(predictions, ratings_test))\r\n",
        "print(\"\\n\")\r\n",
        "print('RMSE=%f' %get_rmse(predictions, ratings_test))\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE=0.700948\n",
            "\n",
            "\n",
            "RMSE=0.875552\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}